{
    "contents" : "library(tm)\nlibrary(lda)\nlibrary(LDAvis)\nlibrary(stringi)\n\nhablaMty <- Corpus(DirSource(\"~/hablaMty/TEXT\"),\n                   readerControl = list(reader = readPlain,\n                                        language = \"spa\",\n                                        load = FALSE,\n                                        encoding = \"UTF-8\"))\n\n##### Unicode fixing #####\nfixUnicode<- function (x){\n    x<-stri_escape_unicode(x)\n    x<-gsub(\"\\\\u00c3\\\\u00a1\", \"\\\\u00e1\", x, fixed = TRUE) # a w acute accent\n    x<-gsub(\"\\\\u00c3\\\\u00a9\", \"\\\\u00e9\", x, fixed = TRUE) # e w acute accent\n    x<-gsub(\"\\\\u00c3\\\\u00ad\", \"\\\\u00ed\", x, fixed = TRUE) # i w acute accent\n    x<-gsub(\"\\\\u00c3\\\\u00b3\", \"\\\\u00f3\", x, fixed = TRUE) # o w acute accent\n    x<-gsub(\"\\\\u00c3\\\\u00ba\", \"\\\\u00fa\", x, fixed = TRUE) # u w acute accent \n    x<-gsub(\"\\\\u00c3\\\\u00b1\", \"\\\\u00f1\", x, fixed = TRUE) # n w tilde\n    x<-gsub(\"\\\\u00c2\\\\u00bf\", \"\\\\u00bf\", x, fixed = TRUE) # inverted questionm.\n    x<-gsub(\"\\\\u00c2\\\\u00a1\", \"\\\\u00a1\", x, fixed = TRUE) # inverted bang\n    x<-gsub(\"\\\\u00e2\\\\u20ac\\\\u0153\", \"\\\\u201c\", x, fixed = TRUE) # o.d.quot. m.\n    x<-gsub(\"\\\\u00e2\\\\u20ac\\\\u009d\", \"\\\\u201d\", x, fixed = TRUE) # c.d.quot. m.\n    x<-gsub(\"\\\\u00c3\\\\u00bc\", \"\\\\u00fc\", x, fixed = TRUE) # u w dieresis\n    x<-gsub(\"\\\\u00e2\\\\u20ac\\\\u2122\", \"'\", x, fixed = TRUE) # u w dieresis\n    x<-stri_unescape_unicode(x)\n    return(x)\n}\n\nhablaMty <- tm_map(hablaMty, content_transformer(fixUnicode))\n\n###### Transformations #######\nf <- content_transformer(function(x, pattern) gsub(pattern, \"\", x))\nhablaMty <- tm_map(hablaMty, f, \"<.*>\") # get rid of annotations\n\n###### Stopwords #####\npalabrasParo <-read.table(\"~/hablaMty/stopwords/palabrasParo.txt\", \n                          encoding = \"UTF-8\", colClasses = \"character\")\n\ntdm <- TermDocumentMatrix(hablaMty,\n            control = list(removePunctuation = TRUE,\n                    stopwords = c(\"pos\",\"allá\",\"digo\",\"nomás\",\"mjm\", palabrasParo[,1]),\n                    removeNumbers = TRUE, tolower = TRUE,\n                    content_transformer(stripWhitespace)))\n\n\ntdm <- removeSparseTerms(tdm, .25)\n\nm <- as.matrix(tdm)\n\n\n\n##### Pruebas ######\nwriteLines(as.character(hablaMty[[1]]))\n\n\n\n# MCMC and model tuning parameters:\nK <- 20\nG <- 5000\nalpha <- 0.02\neta <- 0.02\n\n# Fit the model:\nlibrary(lda)\nset.seed(357)\nt1 <- Sys.time()\nfit <- lda.collapsed.gibbs.sampler(documents = variable.names(m), K = K, vocab = row.names(m), \n                                   num.iterations = G, alpha = alpha, \n                                   eta = eta, initial = NULL, burnin = 0,\n                                   compute.log.likelihood = TRUE)\nt2 <- Sys.time()\nt2 - t1  # about 24 minutes on laptop\n\ntheta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))\nphi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))\n\nMovieReviews <- list(phi = phi,\n                     theta = theta,\n                     doc.length = doc.length,\n                     vocab = vocab,\n                     term.frequency = term.frequency)\n\nlibrary(LDAvis)\n\n# create the JSON object to feed the visualization:\njson <- createJSON(phi = MovieReviews$phi, \n                   theta = MovieReviews$theta, \n                   doc.length = MovieReviews$doc.length, \n                   vocab = MovieReviews$vocab, \n                   term.frequency = MovieReviews$term.frequency)\n\nserVis(json, out.dir = 'vis', open.browser = FALSE)\n",
    "created" : 1441042729803.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1782575299",
    "id" : "61ADF8C9",
    "lastKnownWriteTime" : 1441318501,
    "path" : "~/hablaMty/ldaVis.R",
    "project_path" : "ldaVis.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "type" : "r_source"
}