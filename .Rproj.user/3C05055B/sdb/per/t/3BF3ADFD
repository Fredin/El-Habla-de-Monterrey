{
    "contents" : "#### Taken from http://cpsievert.github.io/LDAvis/reviews/reviews.html by Carson Sievert\n\n# LDAvisData can be installed from GitHub via 'devtools::install_github(\"cpsievert/LDAvisData\")'\ndevtools::install_github(\"cpsievert/LDAvisData\")\ndata(reviews, package = \"LDAvisData\")\n\n# read in some stopwords:\nlibrary(tm)\nstop_words <- stopwords(\"SMART\")\n\n# pre-processing:\nreviews <- gsub(\"'\", \"\", reviews)  # remove apostrophes\nreviews <- gsub(\"[[:punct:]]\", \" \", reviews)  # replace punctuation with space\nreviews <- gsub(\"[[:cntrl:]]\", \" \", reviews)  # replace control characters with space\nreviews <- gsub(\"^[[:space:]]+\", \"\", reviews) # remove whitespace at beginning of documents\nreviews <- gsub(\"[[:space:]]+$\", \"\", reviews) # remove whitespace at end of documents\nreviews <- tolower(reviews)  # force to lowercase\n\n# tokenize on space and output as a list:\ndoc.list <- strsplit(reviews, \"[[:space:]]+\")\n\n# compute the table of terms:\nterm.table <- table(unlist(doc.list))\nterm.table <- sort(term.table, decreasing = TRUE)\n\n# remove terms that are stop words or occur fewer than 5 times:\ndel <- names(term.table) %in% stop_words | term.table < 5\nterm.table <- term.table[!del]\nvocab <- names(term.table)\n\n# now put the documents into the format required by the lda package:\nget.terms <- function(x) {\n    index <- match(x, vocab)\n    index <- index[!is.na(index)]\n    rbind(as.integer(index - 1), as.integer(rep(1, length(index))))\n}\ndocuments <- lapply(doc.list, get.terms)\n\n# Compute some statistics related to the data set:\nD <- length(documents)  # number of documents (2,000)\nW <- length(vocab)  # number of terms in the vocab (14,568)\ndoc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]\nN <- sum(doc.length)  # total number of tokens in the data (546,827)\nterm.frequency <- as.integer(term.table)  # frequencies of terms in the corpus [8939, 5544, 2411, 2410, 2143, ...]\n\n# MCMC and model tuning parameters:\nK <- 20\nG <- 5000\nalpha <- 0.02\neta <- 0.02\n\n# Fit the model:\nlibrary(lda)\nset.seed(357)\nt1 <- Sys.time()\nfit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab, \n                                   num.iterations = G, alpha = alpha, \n                                   eta = eta, initial = NULL, burnin = 0,\n                                   compute.log.likelihood = TRUE)\nt2 <- Sys.time()\nt2 - t1  # about 24 minutes on laptop\n\ntheta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))\nphi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))\n\nMovieReviews <- list(phi = phi,\n                     theta = theta,\n                     doc.length = doc.length,\n                     vocab = vocab,\n                     term.frequency = term.frequency)\n\nlibrary(LDAvis)\n\n# create the JSON object to feed the visualization:\njson <- createJSON(phi = MovieReviews$phi, \n                   theta = MovieReviews$theta, \n                   doc.length = MovieReviews$doc.length, \n                   vocab = MovieReviews$vocab, \n                   term.frequency = MovieReviews$term.frequency)\n\nserVis(json, out.dir = 'vis', open.browser = FALSE)",
    "created" : 1441043052511.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4045806026",
    "id" : "3BF3ADFD",
    "lastKnownWriteTime" : 1441313736,
    "path" : "~/hablaMty/ldaEx.R",
    "project_path" : "ldaEx.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 4,
    "source_on_save" : true,
    "type" : "r_source"
}