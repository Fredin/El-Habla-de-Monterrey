return(x)
}
f <- content_transformer(function(x, pattern) gsub(pattern, "", x))
###### Transformations #######
hablaMty <- tm_map(hablaMty, f, "^E:.*") # get rid of interviewer
hablaMty <- tm_map(hablaMty, f, "<.*>") # get rid of annotations
hablaMty <- tm_map(hablaMty, content_transformer(fixUnicode))
h11 <- hablaMty[meta(hablaMty, tag = "sexo") == "H" & meta(hablaMty, tag = "grupo_edad") == "1" & meta(hablaMty, tag = "nivel_edu") == "1"]
h11
h11 <- hablaMty[meta(hablaMty, tag = "sexo") == "H" &
meta(hablaMty, tag = "grupo_edad") == "1" &
meta(hablaMty, tag = "nivel_edu") == "1"]
h11 <- hablaMty[meta(hablaMty, tag = "sexo") == "H" &
meta(hablaMty, tag = "grupo_edad") == "1" &
meta(hablaMty, tag = "nivel_edu") == "1"]
h12 <- hablaMty[meta(hablaMty, tag = "sexo") == "H" &
meta(hablaMty, tag = "grupo_edad") == "1" &
meta(hablaMty, tag = "nivel_edu") == "2"]
h13 <- hablaMty[meta(hablaMty, tag = "sexo") == "H" &
meta(hablaMty, tag = "grupo_edad") == "1" &
meta(hablaMty, tag = "nivel_edu") == "3"]
m11 <- hablaMty[meta(hablaMty, tag = "sexo") == "M" &
meta(hablaMty, tag = "grupo_edad") == "1" &
meta(hablaMty, tag = "nivel_edu") == "1"]
m12 <- hablaMty[meta(hablaMty, tag = "sexo") == "M" &
meta(hablaMty, tag = "grupo_edad") == "1" &
meta(hablaMty, tag = "nivel_edu") == "2"]
m13 <- hablaMty[meta(hablaMty, tag = "sexo") == "M" &
meta(hablaMty, tag = "grupo_edad") == "1" &
meta(hablaMty, tag = "nivel_edu") == "3"]
m12
104/6
108/6
combinations
comb(c("H","M","1","2","3"),3)
combn(c("H","M","1","2","3"),3)
combn(c("Genero","Edad","Edu"),3)
h11 <- hablaMty[meta(hablaMty, tag = "sexo") == "H" &
meta(hablaMty, tag = "grupo_edad") == "1" &
meta(hablaMty, tag = "nivel_edu") == "1"]
h12 <- hablaMty[meta(hablaMty, tag = "sexo") == "H" &
meta(hablaMty, tag = "grupo_edad") == "1" &
meta(hablaMty, tag = "nivel_edu") == "2"]
h13 <- hablaMty[meta(hablaMty, tag = "sexo") == "H" &
meta(hablaMty, tag = "grupo_edad") == "1" &
meta(hablaMty, tag = "nivel_edu") == "3"]
h21 <- hablaMty[meta(hablaMty, tag = "sexo") == "H" &
meta(hablaMty, tag = "grupo_edad") == "2" &
meta(hablaMty, tag = "nivel_edu") == "1"]
h22 <- hablaMty[meta(hablaMty, tag = "sexo") == "H" &
meta(hablaMty, tag = "grupo_edad") == "2" &
meta(hablaMty, tag = "nivel_edu") == "2"]
h23 <- hablaMty[meta(hablaMty, tag = "sexo") == "H" &
meta(hablaMty, tag = "grupo_edad") == "2" &
meta(hablaMty, tag = "nivel_edu") == "3"]
h31 <- hablaMty[meta(hablaMty, tag = "sexo") == "H" &
meta(hablaMty, tag = "grupo_edad") == "3" &
meta(hablaMty, tag = "nivel_edu") == "1"]
h32 <- hablaMty[meta(hablaMty, tag = "sexo") == "H" &
meta(hablaMty, tag = "grupo_edad") == "3" &
meta(hablaMty, tag = "nivel_edu") == "2"]
h33 <- hablaMty[meta(hablaMty, tag = "sexo") == "H" &
meta(hablaMty, tag = "grupo_edad") == "3" &
meta(hablaMty, tag = "nivel_edu") == "3"]
m11 <- hablaMty[meta(hablaMty, tag = "sexo") == "M" &
meta(hablaMty, tag = "grupo_edad") == "1" &
meta(hablaMty, tag = "nivel_edu") == "1"]
m12 <- hablaMty[meta(hablaMty, tag = "sexo") == "M" &
meta(hablaMty, tag = "grupo_edad") == "1" &
meta(hablaMty, tag = "nivel_edu") == "2"]
m13 <- hablaMty[meta(hablaMty, tag = "sexo") == "M" &
meta(hablaMty, tag = "grupo_edad") == "1" &
meta(hablaMty, tag = "nivel_edu") == "3"]
m21 <- hablaMty[meta(hablaMty, tag = "sexo") == "M" &
meta(hablaMty, tag = "grupo_edad") == "2" &
meta(hablaMty, tag = "nivel_edu") == "1"]
m22 <- hablaMty[meta(hablaMty, tag = "sexo") == "M" &
meta(hablaMty, tag = "grupo_edad") == "2" &
meta(hablaMty, tag = "nivel_edu") == "2"]
m23 <- hablaMty[meta(hablaMty, tag = "sexo") == "M" &
meta(hablaMty, tag = "grupo_edad") == "2" &
meta(hablaMty, tag = "nivel_edu") == "3"]
m31 <- hablaMty[meta(hablaMty, tag = "sexo") == "M" &
meta(hablaMty, tag = "grupo_edad") == "3" &
meta(hablaMty, tag = "nivel_edu") == "1"]
m32 <- hablaMty[meta(hablaMty, tag = "sexo") == "M" &
meta(hablaMty, tag = "grupo_edad") == "3" &
meta(hablaMty, tag = "nivel_edu") == "2"]
m33 <- hablaMty[meta(hablaMty, tag = "sexo") == "M" &
meta(hablaMty, tag = "grupo_edad") == "3" &
meta(hablaMty, tag = "nivel_edu") == "3"]
grupos <- list(h11, h12, h13, h21, h22, h23, h31, h32, h33, m11, m12, m13, m21,
m22, m23, m31, m32, m33)
nombres <- c("h11", "h12", "h13", "h21", "h22", "h23", "h31", "h32", "h33",
"m11", "m12", "m13", "m21", "m22", "m23", "m31", "m32", "m33")
palabrasParo <-read.table("~/hablaMty/stopwords/palabrasParo.txt",
encoding = "UTF-8", colClasses = "character")
for (i in seq_along(grupos)){
tdm <- TermDocumentMatrix(grupos[[i]],
control = list(removePunctuation = TRUE,
stopwords = c("pos","allá","digo","nomás","mjm", palabrasParo),
removeNumbers = TRUE, tolower = TRUE,
content_transformer(stripWhitespace)))
m <- as.matrix(tdm)
word_freqs <- sort(rowSums(m), decreasing = TRUE)
dm <- data.frame(word = names(word_freqs), freq = word_freqs)
top <- dm[1:100,]
png(paste0("~/hablaMty/img/",nombres[i],".png"), width=12, height=8,
units="in", res = 300)
wordcloud(top$word, top$freq, random.order=FALSE,
colors = brewer.pal(8, "Dark2"))
dev.off()
}
palabrasParo1
palabrasParo <-read.table("~/hablaMty/stopwords/palabrasParo.txt",
encoding = "UTF-8", colClasses = "character")
for (i in seq_along(grupos)){
tdm <- TermDocumentMatrix(grupos[[i]],
control = list(removePunctuation = TRUE,
stopwords = c("pos","allá","digo","nomás","mjm", palabrasParo),
removeNumbers = TRUE, tolower = TRUE,
content_transformer(stripWhitespace)))
m <- as.matrix(tdm)
word_freqs <- sort(rowSums(m), decreasing = TRUE)
dm <- data.frame(word = names(word_freqs), freq = word_freqs)
top <- dm[1:100,]
png(paste0("~/hablaMty/img/",nombres[i],".png"), width=12, height=8,
units="in", res = 300)
wordcloud(top$word, top$freq, random.order=FALSE,
colors = brewer.pal(8, "Dark2"))
dev.off()
}
palabrasParo1
palabrasParo
class(palabrasParo)
class(palabrasParo[,1])
for (i in seq_along(grupos)){
tdm <- TermDocumentMatrix(grupos[[i]],
control = list(removePunctuation = TRUE,
stopwords = c("pos","allá","digo","nomás","mjm", palabrasParo[,1]),
removeNumbers = TRUE, tolower = TRUE,
content_transformer(stripWhitespace)))
m <- as.matrix(tdm)
word_freqs <- sort(rowSums(m), decreasing = TRUE)
dm <- data.frame(word = names(word_freqs), freq = word_freqs)
top <- dm[1:100,]
png(paste0("~/hablaMty/img/",nombres[i],".png"), width=12, height=8,
units="in", res = 300)
wordcloud(top$word, top$freq, random.order=FALSE,
colors = brewer.pal(8, "Dark2"))
dev.off()
}
findAssocs(tdm,tdm[5,])
findAssocs(tdm,tdm[5,], 0.6)
findAssocs(tdm,"gente", 0.6)
findAssocs(tdm,"pinche", 0.6)
?match
library(tm)
library(lda)
library(LDAvis)
hablaMty <- Corpus(DirSource("~/hablaMty/TEXT"),
readerControl = list(reader = readPlain,
language = "spa",
load = FALSE,
encoding = "UTF-8"))
##### Unicode fixing #####
fixUnicode<- function (x){
x<-stri_escape_unicode(x)
x<-gsub("\\u00c3\\u00a1", "\\u00e1", x, fixed = TRUE) # a w acute accent
x<-gsub("\\u00c3\\u00a9", "\\u00e9", x, fixed = TRUE) # e w acute accent
x<-gsub("\\u00c3\\u00ad", "\\u00ed", x, fixed = TRUE) # i w acute accent
x<-gsub("\\u00c3\\u00b3", "\\u00f3", x, fixed = TRUE) # o w acute accent
x<-gsub("\\u00c3\\u00ba", "\\u00fa", x, fixed = TRUE) # u w acute accent
x<-gsub("\\u00c3\\u00b1", "\\u00f1", x, fixed = TRUE) # n w tilde
x<-gsub("\\u00c2\\u00bf", "\\u00bf", x, fixed = TRUE) # inverted questionm.
x<-gsub("\\u00c2\\u00a1", "\\u00a1", x, fixed = TRUE) # inverted bang
x<-gsub("\\u00e2\\u20ac\\u0153", "\\u201c", x, fixed = TRUE) # o.d.quot. m.
x<-gsub("\\u00e2\\u20ac\\u009d", "\\u201d", x, fixed = TRUE) # c.d.quot. m.
x<-gsub("\\u00c3\\u00bc", "\\u00fc", x, fixed = TRUE) # u w dieresis
x<-gsub("\\u00e2\\u20ac\\u2122", "'", x, fixed = TRUE) # u w dieresis
x<-stri_unescape_unicode(x)
return(x)
}
hablaMty <- tm_map(hablaMty, content_transformer(fixUnicode))
###### Transformations #######
f <- content_transformer(function(x, pattern) gsub(pattern, "", x))
hablaMty <- tm_map(hablaMty, f, "<.*>") # get rid of annotations
###### Stopwords #####
palabrasParo <-read.table("~/hablaMty/stopwords/palabrasParo.txt",
encoding = "UTF-8", colClasses = "character")
tdm <- TermDocumentMatrix(hablaMty,
control = list(removePunctuation = TRUE,
stopwords = c("pos","allá","digo","nomás","mjm", palabrasParo[,1]),
removeNumbers = TRUE, tolower = TRUE,
content_transformer(stripWhitespace)))
library(stringi)
##### Unicode fixing #####
fixUnicode<- function (x){
x<-stri_escape_unicode(x)
x<-gsub("\\u00c3\\u00a1", "\\u00e1", x, fixed = TRUE) # a w acute accent
x<-gsub("\\u00c3\\u00a9", "\\u00e9", x, fixed = TRUE) # e w acute accent
x<-gsub("\\u00c3\\u00ad", "\\u00ed", x, fixed = TRUE) # i w acute accent
x<-gsub("\\u00c3\\u00b3", "\\u00f3", x, fixed = TRUE) # o w acute accent
x<-gsub("\\u00c3\\u00ba", "\\u00fa", x, fixed = TRUE) # u w acute accent
x<-gsub("\\u00c3\\u00b1", "\\u00f1", x, fixed = TRUE) # n w tilde
x<-gsub("\\u00c2\\u00bf", "\\u00bf", x, fixed = TRUE) # inverted questionm.
x<-gsub("\\u00c2\\u00a1", "\\u00a1", x, fixed = TRUE) # inverted bang
x<-gsub("\\u00e2\\u20ac\\u0153", "\\u201c", x, fixed = TRUE) # o.d.quot. m.
x<-gsub("\\u00e2\\u20ac\\u009d", "\\u201d", x, fixed = TRUE) # c.d.quot. m.
x<-gsub("\\u00c3\\u00bc", "\\u00fc", x, fixed = TRUE) # u w dieresis
x<-gsub("\\u00e2\\u20ac\\u2122", "'", x, fixed = TRUE) # u w dieresis
x<-stri_unescape_unicode(x)
return(x)
}
hablaMty <- tm_map(hablaMty, content_transformer(fixUnicode))
###### Transformations #######
f <- content_transformer(function(x, pattern) gsub(pattern, "", x))
hablaMty <- tm_map(hablaMty, f, "<.*>") # get rid of annotations
###### Stopwords #####
palabrasParo <-read.table("~/hablaMty/stopwords/palabrasParo.txt",
encoding = "UTF-8", colClasses = "character")
tdm <- TermDocumentMatrix(hablaMty,
control = list(removePunctuation = TRUE,
stopwords = c("pos","allá","digo","nomás","mjm", palabrasParo[,1]),
removeNumbers = TRUE, tolower = TRUE,
content_transformer(stripWhitespace)))
writeLines(hablaMty[[1]])
writeLines(as.character(hablaMty[[1]]))
??tm
inspect(tdm[5:10,])
inspect(tdm[1:10,1:3])
removeSparseTerms(tdm, 0.4)
?removeSparseTerms
removeSparseTerms(tdm, 0.25)
library(tm)
removeSparseTerms(tdm, 0.25)
removeSparseTerms(tdm, 0.50)
removeSparseTerms(tdm, 1)
removeSparseTerms(tdm, .9)
removeSparseTerms(tdm, .1)
tdm <- removeSparseTerms(tdm, .25)
108*.25
#### Taken from http://cpsievert.github.io/LDAvis/reviews/reviews.html by Carson Sievert
# LDAvisData can be installed from GitHub via 'devtools::install_github("cpsievert/LDAvisData")'
devtools::install_github("cpsievert/LDAvisData")
data(reviews, package = "LDAvisData")
# read in some stopwords:
library(tm)
stop_words <- stopwords("SMART")
# pre-processing:
reviews <- gsub("'", "", reviews)  # remove apostrophes
reviews <- gsub("[[:punct:]]", " ", reviews)  # replace punctuation with space
reviews <- gsub("[[:cntrl:]]", " ", reviews)  # replace control characters with space
reviews <- gsub("^[[:space:]]+", "", reviews) # remove whitespace at beginning of documents
reviews <- gsub("[[:space:]]+$", "", reviews) # remove whitespace at end of documents
reviews <- tolower(reviews)  # force to lowercase
# tokenize on space and output as a list:
doc.list <- strsplit(reviews, "[[:space:]]+")
# compute the table of terms:
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
# remove terms that are stop words or occur fewer than 5 times:
del <- names(term.table) %in% stop_words | term.table < 5
term.table <- term.table[!del]
vocab <- names(term.table)
# now put the documents into the format required by the lda package:
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
# Compute some statistics related to the data set:
D <- length(documents)  # number of documents (2,000)
W <- length(vocab)  # number of terms in the vocab (14,568)
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # total number of tokens in the data (546,827)
term.frequency <- as.integer(term.table)  # frequencies of terms in the corpus [8939, 5544, 2411, 2410, 2143, ...]
# MCMC and model tuning parameters:
K <- 20
G <- 5000
alpha <- 0.02
eta <- 0.02
# Fit the model:
library(lda)
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
t2 <- Sys.time()
t2 - t1  # about 24 minutes on laptop
theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
MovieReviews <- list(phi = phi,
theta = theta,
doc.length = doc.length,
vocab = vocab,
term.frequency = term.frequency)
library(LDAvis)
# create the JSON object to feed the visualization:
json <- createJSON(phi = MovieReviews$phi,
theta = MovieReviews$theta,
doc.length = MovieReviews$doc.length,
vocab = MovieReviews$vocab,
term.frequency = MovieReviews$term.frequency)
serVis(json, out.dir = 'vis', open.browser = FALSE)
install.packages("servr")
library(lda)
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
t2 <- Sys.time()
t2 - t1  # about 24 minutes on laptop
theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
MovieReviews <- list(phi = phi,
theta = theta,
doc.length = doc.length,
vocab = vocab,
term.frequency = term.frequency)
library(LDAvis)
# create the JSON object to feed the visualization:
json <- createJSON(phi = MovieReviews$phi,
theta = MovieReviews$theta,
doc.length = MovieReviews$doc.length,
vocab = MovieReviews$vocab,
term.frequency = MovieReviews$term.frequency)
serVis(json, out.dir = 'vis', open.browser = FALSE)
theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
MovieReviews <- list(phi = phi,
theta = theta,
doc.length = doc.length,
vocab = vocab,
term.frequency = term.frequency)
library(LDAvis)
library(servr)
# create the JSON object to feed the visualization:
json <- createJSON(phi = MovieReviews$phi,
theta = MovieReviews$theta,
doc.length = MovieReviews$doc.length,
vocab = MovieReviews$vocab,
term.frequency = MovieReviews$term.frequency)
serVis(json, out.dir = 'vis', open.browser = FALSE)
library(tm)
library(lda)
library(LDAvis)
library(stringi)
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
t2 <- Sys.time()
t2 - t1  # about 24 minutes on laptop
theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
MovieReviews <- list(phi = phi,
theta = theta,
doc.length = doc.length,
vocab = vocab,
term.frequency = term.frequency)
library(LDAvis)
# create the JSON object to feed the visualization:
json <- createJSON(phi = MovieReviews$phi,
theta = MovieReviews$theta,
doc.length = MovieReviews$doc.length,
vocab = MovieReviews$vocab,
term.frequency = MovieReviews$term.frequency)
serVis(json, out.dir = 'vis', open.browser = FALSE)
source('~/hablaMty/ldaEx.R')
source('~/hablaMty/ldaEx.R')
install|("devtools")
install.packages("devtools")
devtools::install_github("cpsievert/LDAvisData")
data(reviews, package = "LDAvisData")
# read in some stopwords:
library(tm)
stop_words <- stopwords("SMART")
# pre-processing:
reviews <- gsub("'", "", reviews)  # remove apostrophes
reviews <- gsub("[[:punct:]]", " ", reviews)  # replace punctuation with space
reviews <- gsub("[[:cntrl:]]", " ", reviews)  # replace control characters with space
reviews <- gsub("^[[:space:]]+", "", reviews) # remove whitespace at beginning of documents
reviews <- gsub("[[:space:]]+$", "", reviews) # remove whitespace at end of documents
reviews <- tolower(reviews)  # force to lowercase
# tokenize on space and output as a list:
doc.list <- strsplit(reviews, "[[:space:]]+")
# compute the table of terms:
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
# remove terms that are stop words or occur fewer than 5 times:
del <- names(term.table) %in% stop_words | term.table < 5
term.table <- term.table[!del]
vocab <- names(term.table)
# now put the documents into the format required by the lda package:
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
# Compute some statistics related to the data set:
D <- length(documents)  # number of documents (2,000)
W <- length(vocab)  # number of terms in the vocab (14,568)
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # total number of tokens in the data (546,827)
term.frequency <- as.integer(term.table)  # frequencies of terms in the corpus [8939, 5544, 2411, 2410, 2143, ...]
# MCMC and model tuning parameters:
K <- 20
G <- 5000
alpha <- 0.02
eta <- 0.02
# Fit the model:
library(lda)
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
t2 <- Sys.time()
t2 - t1  # about 24 minutes on laptop
theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
MovieReviews <- list(phi = phi,
theta = theta,
doc.length = doc.length,
vocab = vocab,
term.frequency = term.frequency)
library(LDAvis)
# create the JSON object to feed the visualization:
json <- createJSON(phi = MovieReviews$phi,
theta = MovieReviews$theta,
doc.length = MovieReviews$doc.length,
vocab = MovieReviews$vocab,
term.frequency = MovieReviews$term.frequency)
serVis(json, out.dir = 'vis', open.browser = FALSE)
install.packages("lda")
install.packages("LDAvis")
source('~/hablaMty/ldaEx.R')
head(vocab)
head(documents)
head(term.table)
term.table
hablaMty <- Corpus(DirSource("~/hablaMty/TEXT"),
readerControl = list(reader = readPlain,
language = "spa",
load = FALSE,
encoding = "UTF-8"))
##### Unicode fixing #####
fixUnicode<- function (x){
x<-stri_escape_unicode(x)
x<-gsub("\\u00c3\\u00a1", "\\u00e1", x, fixed = TRUE) # a w acute accent
x<-gsub("\\u00c3\\u00a9", "\\u00e9", x, fixed = TRUE) # e w acute accent
x<-gsub("\\u00c3\\u00ad", "\\u00ed", x, fixed = TRUE) # i w acute accent
x<-gsub("\\u00c3\\u00b3", "\\u00f3", x, fixed = TRUE) # o w acute accent
x<-gsub("\\u00c3\\u00ba", "\\u00fa", x, fixed = TRUE) # u w acute accent
x<-gsub("\\u00c3\\u00b1", "\\u00f1", x, fixed = TRUE) # n w tilde
x<-gsub("\\u00c2\\u00bf", "\\u00bf", x, fixed = TRUE) # inverted questionm.
x<-gsub("\\u00c2\\u00a1", "\\u00a1", x, fixed = TRUE) # inverted bang
x<-gsub("\\u00e2\\u20ac\\u0153", "\\u201c", x, fixed = TRUE) # o.d.quot. m.
x<-gsub("\\u00e2\\u20ac\\u009d", "\\u201d", x, fixed = TRUE) # c.d.quot. m.
x<-gsub("\\u00c3\\u00bc", "\\u00fc", x, fixed = TRUE) # u w dieresis
x<-gsub("\\u00e2\\u20ac\\u2122", "'", x, fixed = TRUE) # u w dieresis
x<-stri_unescape_unicode(x)
return(x)
}
hablaMty <- tm_map(hablaMty, content_transformer(fixUnicode))
###### Transformations #######
f <- content_transformer(function(x, pattern) gsub(pattern, "", x))
hablaMty <- tm_map(hablaMty, f, "<.*>") # get rid of annotations
###### Stopwords #####
palabrasParo <-read.table("~/hablaMty/stopwords/palabrasParo.txt",
encoding = "UTF-8", colClasses = "character")
tdm <- TermDocumentMatrix(hablaMty,
control = list(removePunctuation = TRUE,
stopwords = c("pos","allá","digo","nomás","mjm", palabrasParo[,1]),
removeNumbers = TRUE, tolower = TRUE,
content_transformer(stripWhitespace)))
tdm <- removeSparseTerms(tdm, .25)
head(tdm)
tdm
m <- as.matrix(tdm)
head(m)
word_freqs <- sort(rowSums(m), decreasing = TRUE)
head(word_freqs)
dm <- data.frame(word = names(word_freqs), freq = word_freqs)
head(dm)
row.names(m)
column.names(m)
names(m)
var.names(m)
?row.names
names(m)
m[1,]
variable.names(m)
library(lda)
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = variable.names(m), K = K, vocab = row.names(m),
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
t2 <- Sys.time()
t2 - t1  # about 24 minutes on laptop
